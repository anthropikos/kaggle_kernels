{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0c4214",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-01T04:05:30.091003Z",
     "iopub.status.busy": "2024-05-01T04:05:30.089488Z",
     "iopub.status.idle": "2024-05-01T04:05:31.536603Z",
     "shell.execute_reply": "2024-05-01T04:05:31.533466Z"
    },
    "papermill": {
     "duration": 1.460038,
     "end_time": "2024-05-01T04:05:31.540521",
     "exception": false,
     "start_time": "2024-05-01T04:05:30.080483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/learn-ai-bbc/BBC News Train.csv\n",
      "/kaggle/input/learn-ai-bbc/BBC News Sample Solution.csv\n",
      "/kaggle/input/learn-ai-bbc/BBC News Test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirpath, dirnames, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f334be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T04:05:31.558394Z",
     "iopub.status.busy": "2024-05-01T04:05:31.556960Z",
     "iopub.status.idle": "2024-05-01T04:05:32.742646Z",
     "shell.execute_reply": "2024-05-01T04:05:32.740876Z"
    },
    "papermill": {
     "duration": 1.198798,
     "end_time": "2024-05-01T04:05:32.746308",
     "exception": false,
     "start_time": "2024-05-01T04:05:31.547510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input\r\n",
      "/kaggle/input/learn-ai-bbc\r\n",
      "/kaggle/input/learn-ai-bbc/BBC News Train.csv\r\n",
      "/kaggle/input/learn-ai-bbc/BBC News Sample Solution.csv\r\n",
      "/kaggle/input/learn-ai-bbc/BBC News Test.csv\r\n"
     ]
    }
   ],
   "source": [
    "!find /kaggle/input -regex '.*'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4b733",
   "metadata": {
    "papermill": {
     "duration": 0.006624,
     "end_time": "2024-05-01T04:05:32.759992",
     "exception": false,
     "start_time": "2024-05-01T04:05:32.753368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comparing [some] Unsupervised- and Supervised-Classification on BBC News Articles\n",
    "\n",
    "## Goal\n",
    "- Answer the question - \"Are Matrix Factorization and SVD the same?\"\n",
    "- Use some libraries/frameworks that I am unfamiliar with - Scikit-Learn, NLTK, spaCy.\n",
    "- Compare at least one algorithm of Unsupervised and Supervised methods each!\n",
    "- Write a notebook that is concise, contains lots of figures, and allowing others to learn and build-upon.\n",
    "\n",
    "## Outline\n",
    "- EDA\n",
    "- Unsupervised Classification\n",
    "    - Libraries: sklearn, nltk, spacy\n",
    "    - Tokenize -> Lemmatized -> Cleaned of extra words (i.e., stop words) -> NMF\n",
    "    - Just for fun, add PCA and SVD to compare\n",
    "- Supervised Classification - Include the labels!!!\n",
    "    - SVM, RF, LogReg, LSTM, Ensembling, NaiveBayes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1eceff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T04:05:32.778224Z",
     "iopub.status.busy": "2024-05-01T04:05:32.777628Z",
     "iopub.status.idle": "2024-05-01T04:05:42.968299Z",
     "shell.execute_reply": "2024-05-01T04:05:42.966562Z"
    },
    "papermill": {
     "duration": 10.205842,
     "end_time": "2024-05-01T04:05:42.972416",
     "exception": false,
     "start_time": "2024-05-01T04:05:32.766574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, ClusterMixin\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from itertools import permutations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "## TODO: \n",
    "## x Create data reading, merging, and dataframe transforming function\n",
    "## x Create spaCy transformer class for pipeline\n",
    "## x Create NMF transformer class for pipeline\n",
    "## x Create classifying step (permute and find the best match) for the pipeline\n",
    "## x Add score method to the classifier (will be exposed at the end of the pipeline)\n",
    "## x Add a todo to further test/explore how the feature_out method/attribute work.\n",
    "\n",
    "## Outline: \n",
    "## - USL\n",
    "##   - NMF categorization\n",
    "##     - Pipeline := (extract, transform (TFIDF embedding), NMF (factorization), classifying (permute for best mapping))\n",
    "##     - Pipeline last component (classifier) implement sklearn.metrics.classification_report and sklearn.metrics.confusion_matrix\n",
    "##     - Further Test/Explore: ??? How the feature_out thing works.\n",
    "## - SL\n",
    "##   - LinearSVC vs SGD(hinge-loss), LogisticRegression vs SGD(log_loss) - Interested in looking at the differences in their optimization methods since SGD is just an optimization method.\n",
    "##   - Pipeline := (same as prior but replace the last classifier)\n",
    "\n",
    "## Helper functions\n",
    "def limitlessPandas(func):\n",
    "    \"\"\"My weriding/non-Pythonic way of making Pandas dataframe display more pleasing\"\"\"\n",
    "    def wrapper(stuff):\n",
    "        with pd.option_context(\"display.max_colwidth\", None, \"display.max_rows\", None):\n",
    "            func(stuff)\n",
    "    return wrapper\n",
    "\n",
    "## Transformers / Classifiers\n",
    "class Dataloader():\n",
    "    \"\"\"Dataloader for convenience.\n",
    "    \n",
    "    This dataloader is for convenience as it encapsulates the reading, merging, \n",
    "    and train_test_split() functionality. The main goal is to provide an abstracted \n",
    "    interface so that the CSV file is easily workable without having to dive into \n",
    "    the structure of the CSV.\n",
    "\n",
    "    Additionally, has a train_test_split() method to ensure proper independence \n",
    "    between training and testing datasets.\n",
    "            \n",
    "    Attributes: \n",
    "        X (list): List of data to be fet into a pipeline.\n",
    "        y (list): List of label corresponding to each data.\n",
    "\n",
    "    Methods: \n",
    "        train_test_split(**kwargs): Takes keyword only arguments accepted by \n",
    "            sklearn.model_selection.train_test_split()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Does not need any arguments.\n",
    "        \n",
    "        This dataloader is custom to this notebook and dataset for convenience and\n",
    "        would not work properly anywhere else.\n",
    "        \"\"\"\n",
    "        self._full_dataframe = self._read_csv_and_merge()\n",
    "        self.X, self.y = self._dataframe_to_X_y(self._full_dataframe)\n",
    "    \n",
    "    def _read_csv_and_merge(self, ): \n",
    "        \"\"\"Reads the three CSVs and merge them into one single dataframe.\"\"\"\n",
    "\n",
    "        ## Read the CSVs\n",
    "        df = pd.read_csv('/kaggle/input/learn-ai-bbc/BBC News Train.csv')\n",
    "        df_testSet = pd.read_csv('/kaggle/input/learn-ai-bbc/BBC News Test.csv')\n",
    "        df_testSetSolution = pd.read_csv('/kaggle/input/learn-ai-bbc/BBC News Sample Solution.csv')\n",
    "\n",
    "        ## Database like join of the testSet and the testSetSolution df\n",
    "        selected_columns = ['ArticleId', 'Text', 'Category']\n",
    "        mergedDf = df_testSet.merge(\n",
    "            right=df_testSetSolution,\n",
    "            how='inner',  # Should not have missing labels, if there are inner-join should exclude them.\n",
    "            on='ArticleId'\n",
    "        )\n",
    "        mergedDf = mergedDf[selected_columns]  # Reorder the columns\n",
    "\n",
    "        ## Concat the training set and the test set\n",
    "        df = pd.concat([df, mergedDf], axis=0, ignore_index=True)\n",
    "\n",
    "        return df  # Training set and test set merged dataframe\n",
    "\n",
    "    def _dataframe_to_X_y(self, dataframe:pd.DataFrame):\n",
    "        \"\"\"Helper to convert the dataframe to X and y lists that is expected by the sklearn functions.\"\"\"\n",
    "        X = dataframe['Text'].to_list()      # Data to a Python list\n",
    "        y = dataframe['Category'].to_list()  # Labels to a Python list\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def train_test_split(self, **kwargs):\n",
    "        \"\"\"Train/test splits the loaded data. Takes keyword only arguments.\"\"\"\n",
    "        return train_test_split(self.X, self.y, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class SpaCy_transformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Transformer utilizing spaCy's pretrained model.\n",
    "    \n",
    "    The goal of this transformer is to also partially reduce the dimensionality \n",
    "    of the documents before using a CountVectorizer and TfidfTransformer. The \n",
    "    idea behind reducing each word to its lemma is to decrease the number of word\n",
    "    variants in a document. \n",
    "    \n",
    "    This could be detrimental in NMF clustering accuracy as each document is now\n",
    "    projected to a lower dimension. \n",
    "    \n",
    "    TODO: \n",
    "        - [ ] Add param to choose whether to lemmatize\n",
    "        \n",
    "    Attributes: \n",
    "    \n",
    "    Methods: \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *, n_process=None, ):\n",
    "        self.n_process = os.cpu_count() if n_process is None else n_process\n",
    "        \n",
    "    def fit(self, X, y=None):  # Transformers are transductive and doesn't need a `y`\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"Filter out, and reduces each word to its lemma (or canonical form).\"\"\"\n",
    "        nlp = spacy.load(\"en_core_web_sm\")  # The trained model used\n",
    "        doc_generator = nlp.pipe(X, n_process=self.n_process)\n",
    "        \n",
    "        doc_list = []\n",
    "        for doc in doc_generator: \n",
    "            lemma_list = []\n",
    "            for token in doc:\n",
    "                ## Some exclusion logic\n",
    "                if token.is_stop | token.is_currency | token.is_space | token.is_punct:\n",
    "                    # Exclude stop words and currency symbols\n",
    "                    continue\n",
    "                if ((token.is_alpha) & (len(token.text)==1)):\n",
    "                    # Exclude the \"s\" - Seems like the dataset has already been processed\n",
    "                    # and resulted in a lot of single alphabet \"s\" when the original author\n",
    "                    # intended as possesive \"'s\"\n",
    "                    continue\n",
    "                lemma_list.append(token.lemma_) # Lemmatization\n",
    "            stringed_lemmas = \" \".join(lemma_list)\n",
    "            doc_list.append(stringed_lemmas)\n",
    "            \n",
    "        return doc_list\n",
    "    \n",
    "    ## The TransformerMixin defines fit_transform() and delegates to fit() and transform()\n",
    "    #def fit_transform(self, X, y=None): \n",
    "    #    self.fit(X, y)\n",
    "    #    return self.transform(X)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Classifier_permute_for_best_mapping(ClassifierMixin, BaseEstimator):\n",
    "    \"\"\"Pure NMF classifier. Using the featrues extracted with NMF to permute the best label mapping.\n",
    "    \n",
    "    This pure NMF classifier is similar to a clustering algorithm that is \n",
    "    transductive. The model is not really able to conduct any transfer learning\n",
    "    as there are not any weights that were learned via training. By limiting the\n",
    "    factorization to an output dimension of 5, which is matchig the number of \n",
    "    categories we have in the dataset, it is similar to doing a 5-NN or PCA of\n",
    "    top 5 principal components.\n",
    "    \n",
    "    The fact that this is transductive makes the design of the methods a little \n",
    "    interesting as I have to think about how to implement fit/predict and have \n",
    "    tasks delegated to them.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, categories=None, normalize=None): \n",
    "        self.categories = ['business', 'tech', 'politics', 'sport', 'entertainment'] if categories==None else categories\n",
    "        self.normalize = True if normalize==None else normalize\n",
    "        self._is_predictted = False  # Private attribute; attr postfixed with _ is reserved for checking if check_is_fitted\n",
    "        #self.best_mapping = dict()\n",
    "        #self.mapping_permute_results = pd.DataFrame()\n",
    "        #self.best_accuracy_score = np.float32()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.is_fitted_ = True\n",
    "\n",
    "    def predict(self, X, y=None, **kwargs): \n",
    "        \"\"\"Given NxK matrix of likelihood and return index of category of max likelihood.\n",
    "        \n",
    "        Given a NxK matrix of probability where there are K categories and return \n",
    "        array of size N of index of K-categories.\n",
    "        \"\"\"\n",
    "        assert X.ndim == 2, 'The training vector has to have 2-dimensions, thus shape of NxD.'\n",
    "        y_pred = np.argmax(X, axis=1)\n",
    "        \n",
    "        ## Having ran fit() then predict() is the same as running fit_predict()\n",
    "        self._is_predictted = True\n",
    "        \n",
    "        ## Whether to search for the best mapping\n",
    "        if y==None:  # True y-label not provided\n",
    "            return y_pred\n",
    "        if y!=None:  # True y-label provided\n",
    "            self._find_best_mapping(y_true=y, y_pred=y_pred, **kwargs)\n",
    "            return y_pred\n",
    "    \n",
    "    def fit_predict(self, X, y=None, **kwargs):\n",
    "        self.fit(X) \n",
    "        y_pred = self.predict(X, y)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def remap_prediction_to_string(self, y_pred):\n",
    "        \"\"\"Remap int y_pred categorical label back to string using the best mapping.\"\"\"\n",
    "        assert self.is_fitted_ & self._is_predictted, \"(`fit()` and `predict()`) or `fit_predict()` has to be called first.\"\n",
    "        \n",
    "        ## Remap y_pred back to string categorical label\n",
    "        mapping = {value:key for key, value in self.best_mapping.items()}\n",
    "        remapped_y_pred = list( map(lambda idx: mapping[idx], y_pred) )\n",
    "        return remapped_y_pred\n",
    "        \n",
    "    \n",
    "    def score(self, X, y, **kwargs):\n",
    "        if self.is_fitted_ & self._is_predictted: \n",
    "            return self.best_accuracy_score\n",
    "        else:\n",
    "            self.fit_predict(X, y, **kwargs)\n",
    "            return self.best_accuracy_score\n",
    "\n",
    "    \n",
    "    def _find_best_mapping(self, y_true, y_pred, categories=None, normalize=True, **kwargs):\n",
    "        \"\"\"Permute the categorie to find the best mapping.\"\"\"\n",
    "        categories = self.categories if categories==None else categories\n",
    "\n",
    "        perms = permutations(categories, len(categories))\n",
    "        track_acc_score = []\n",
    "        track_mapping = []\n",
    "\n",
    "        for perm in perms: \n",
    "            mapping = {key:value for value, key in enumerate(perm)} \n",
    "            true_mapped = [mapping[category] for category in y_true]\n",
    "            acc_score = accuracy_score(y_true=true_mapped, y_pred=y_pred, normalize=normalize)\n",
    "\n",
    "            track_acc_score.append(acc_score)\n",
    "            track_mapping.append(mapping)\n",
    "\n",
    "\n",
    "        result = pd.DataFrame({\"Mapping\": track_mapping, \n",
    "                               \"AccuracyScore\": track_acc_score})\n",
    "        result = result.sort_values(by=\"AccuracyScore\", ascending=False)\n",
    "        result = result.reset_index()\n",
    "        \n",
    "        self.mapping_permute_results = result\n",
    "        self.best_mapping = self.mapping_permute_results.Mapping.iloc[0]\n",
    "        self.best_accuracy_score = self.mapping_permute_results.AccuracyScore.iloc[0]\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8166664c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T04:05:42.988318Z",
     "iopub.status.busy": "2024-05-01T04:05:42.987548Z",
     "iopub.status.idle": "2024-05-01T04:05:46.617139Z",
     "shell.execute_reply": "2024-05-01T04:05:46.615539Z"
    },
    "papermill": {
     "duration": 3.641765,
     "end_time": "2024-05-01T04:05:46.620532",
     "exception": false,
     "start_time": "2024-05-01T04:05:42.978767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mapping:        {'business': 0, 'politics': 1, 'sport': 2, 'tech': 3, 'entertainment': 4}\n",
      "\n",
      "Best accuracy score: 0.6573741007194245\n",
      "\n",
      "NMF Permute and Find best Mapping Results: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Mapping</th>\n",
       "      <th>AccuracyScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>{'business': 0, 'politics': 1, 'sport': 2, 'tech': 3, 'entertainment': 4}</td>\n",
       "      <td>0.657374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>{'business': 0, 'entertainment': 1, 'sport': 2, 'tech': 3, 'politics': 4}</td>\n",
       "      <td>0.473471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>{'business': 0, 'politics': 1, 'sport': 2, 'entertainment': 3, 'tech': 4}</td>\n",
       "      <td>0.465378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>{'business': 0, 'tech': 1, 'sport': 2, 'politics': 3, 'entertainment': 4}</td>\n",
       "      <td>0.464928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>{'politics': 0, 'business': 1, 'sport': 2, 'tech': 3, 'entertainment': 4}</td>\n",
       "      <td>0.439299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0      8   \n",
       "1     22   \n",
       "2      9   \n",
       "3      2   \n",
       "4     50   \n",
       "\n",
       "                                                                     Mapping  \\\n",
       "0  {'business': 0, 'politics': 1, 'sport': 2, 'tech': 3, 'entertainment': 4}   \n",
       "1  {'business': 0, 'entertainment': 1, 'sport': 2, 'tech': 3, 'politics': 4}   \n",
       "2  {'business': 0, 'politics': 1, 'sport': 2, 'entertainment': 3, 'tech': 4}   \n",
       "3  {'business': 0, 'tech': 1, 'sport': 2, 'politics': 3, 'entertainment': 4}   \n",
       "4  {'politics': 0, 'business': 1, 'sport': 2, 'tech': 3, 'entertainment': 4}   \n",
       "\n",
       "   AccuracyScore  \n",
       "0       0.657374  \n",
       "1       0.473471  \n",
       "2       0.465378  \n",
       "3       0.464928  \n",
       "4       0.439299  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################################################\n",
    "### Let's do some NMF Predictions! - Unsupervised\n",
    "################################################################################\n",
    "\n",
    "## NOTES: \n",
    "##   - Holding out one entry as test set to approximate not doing train/test split.\n",
    "##   - The reason for not doing a train/test split is because the way we are using NMF and then\n",
    "##     permutating the y_true for the max accuracy score is essentially like KNN clustering or \n",
    "##     PCA dimensionality reduction. They are all a form of ***transductive inferencing***\n",
    "##     of which there are no generalization based on doing training or \"fitting\" (sklearn term).\n",
    "\n",
    "\n",
    "################################################################################\n",
    "## METHOD 1: Using the pipeline\n",
    "################################################################################\n",
    "\n",
    "## Load the data\n",
    "## NOTE: We are not splitting the train/test data because using NMF this way \n",
    "##   is essentially a clustering method, which is tranductive.\n",
    "dataloader = Dataloader()\n",
    "X_train, X_test, y_train, y_test = dataloader.train_test_split(test_size=1, random_state=550)  # Essentially not splitting\n",
    "\n",
    "## Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    #('lemmatizer',      SpaCy_transformer()),  # Disabling lemmatizer - Too slow~~\n",
    "    ('countVectorizer',  CountVectorizer()),\n",
    "    ('tfidfTransformer', TfidfTransformer()),\n",
    "    ('nmf',              NMF(n_components=5)),\n",
    "    ('classifier',       Classifier_permute_for_best_mapping())\n",
    "])\n",
    "\n",
    "## RUN the pipeline\n",
    "predictions = pipeline.fit_predict(X_train, y_train)\n",
    "\n",
    "## Get the results\n",
    "last_component       = pipeline[-1]\n",
    "permutation_results  = last_component.mapping_permute_results\n",
    "best_mapping         = last_component.best_mapping\n",
    "best_accuracy_score  = last_component.best_accuracy_score\n",
    "remapped_predictions = last_component.remap_prediction_to_string(predictions) # Remap the predictions: integers -> strings\n",
    "\n",
    "## Print the results\n",
    "print( f\"Best mapping:        {best_mapping}\", end='\\n'*2 )\n",
    "print( f\"Best accuracy score: {best_accuracy_score}\", end='\\n'*2 )\n",
    "\n",
    "print( \"NMF Permute and Find best Mapping Results: \" )\n",
    "limitlessPandas(display)(permutation_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f0dc85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T04:05:46.635648Z",
     "iopub.status.busy": "2024-05-01T04:05:46.635161Z",
     "iopub.status.idle": "2024-05-01T04:05:46.642332Z",
     "shell.execute_reply": "2024-05-01T04:05:46.640700Z"
    },
    "papermill": {
     "duration": 0.017703,
     "end_time": "2024-05-01T04:05:46.644710",
     "exception": false,
     "start_time": "2024-05-01T04:05:46.627007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# ## TESTING randomSearchCV\n",
    "# ################################################################################\n",
    "\n",
    "# param_grid = {\n",
    "# #     \"countVectorizer__ngram_range\": [(1,1), (1, 2), (1, 3), (1, 4), (1, 5)],\n",
    "#     \"tfidfTransformer__norm\": [\"l1\", \"l2\"],\n",
    "# #     \"tfidfTransformer__use_idf\": [True, False],\n",
    "# #     \"tfidfTransformer__sublinear_tf\": [True, False],\n",
    "# #     \"nmf__init\": ['random', 'nndsvd', 'nndsvda', 'nndsvdar'], \n",
    "# #     \"nmf__l1_ratio\": np.linspace(0, 1, 5)\n",
    "# }\n",
    "\n",
    "# randomSearchCV = RandomizedSearchCV(\n",
    "#     estimator=pipeline, \n",
    "#     param_distributions=param_grid, \n",
    "#     n_jobs=-1, \n",
    "#     error_score='raise', \n",
    "#     cv=5, \n",
    "#     return_train_score=True)\n",
    "\n",
    "# randomSearchCV.fit(X_train, y_train)\n",
    "\n",
    "# randomSearchCV.cv_results_\n",
    "# randomSearchCV.best_score_\n",
    "# randomSearchCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b11df750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T04:05:46.660282Z",
     "iopub.status.busy": "2024-05-01T04:05:46.659775Z",
     "iopub.status.idle": "2024-05-01T04:05:49.696498Z",
     "shell.execute_reply": "2024-05-01T04:05:49.695034Z"
    },
    "papermill": {
     "duration": 3.048446,
     "end_time": "2024-05-01T04:05:49.700002",
     "exception": false,
     "start_time": "2024-05-01T04:05:46.651556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mapping:        {'business': 0, 'politics': 1, 'sport': 2, 'tech': 3, 'entertainment': 4}\n",
      "\n",
      "Best accuracy score: 0.6573741007194245\n",
      "\n",
      "NMF Permute and Find best Mapping Results: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Mapping</th>\n",
       "      <th>AccuracyScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>{'business': 0, 'politics': 1, 'sport': 2, 'tech': 3, 'entertainment': 4}</td>\n",
       "      <td>0.657374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>{'business': 0, 'entertainment': 1, 'sport': 2, 'tech': 3, 'politics': 4}</td>\n",
       "      <td>0.473471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>{'business': 0, 'politics': 1, 'sport': 2, 'entertainment': 3, 'tech': 4}</td>\n",
       "      <td>0.465378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>{'business': 0, 'tech': 1, 'sport': 2, 'politics': 3, 'entertainment': 4}</td>\n",
       "      <td>0.464928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>{'politics': 0, 'business': 1, 'sport': 2, 'tech': 3, 'entertainment': 4}</td>\n",
       "      <td>0.439299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0      8   \n",
       "1     22   \n",
       "2      9   \n",
       "3      2   \n",
       "4     50   \n",
       "\n",
       "                                                                     Mapping  \\\n",
       "0  {'business': 0, 'politics': 1, 'sport': 2, 'tech': 3, 'entertainment': 4}   \n",
       "1  {'business': 0, 'entertainment': 1, 'sport': 2, 'tech': 3, 'politics': 4}   \n",
       "2  {'business': 0, 'politics': 1, 'sport': 2, 'entertainment': 3, 'tech': 4}   \n",
       "3  {'business': 0, 'tech': 1, 'sport': 2, 'politics': 3, 'entertainment': 4}   \n",
       "4  {'politics': 0, 'business': 1, 'sport': 2, 'tech': 3, 'entertainment': 4}   \n",
       "\n",
       "   AccuracyScore  \n",
       "0       0.657374  \n",
       "1       0.473471  \n",
       "2       0.465378  \n",
       "3       0.464928  \n",
       "4       0.439299  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################################################\n",
    "## Method 2 - The step-wise functional programming way\n",
    "################################################################################\n",
    "\n",
    "## Load the data\n",
    "## NOTE: We are not splitting the train/test data because using NMF this way \n",
    "##   is essentially a clustering method, which is tranductive.\n",
    "dataloader = Dataloader()\n",
    "X_train, X_test, y_train, y_test = dataloader.train_test_split(test_size=1, random_state=550)  # Essentially not splitting\n",
    "\n",
    "## Instantiate pipeline components\n",
    "lemmatizer       = SpaCy_transformer()  # Disabled lemmatizer - Too slow~\n",
    "countVectorizer  = CountVectorizer()\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "nmf              = NMF(n_components=5)\n",
    "classifier       = Classifier_permute_for_best_mapping()\n",
    "\n",
    "## Run through the pipeline step-by-step\n",
    "temp = countVectorizer.fit_transform(X_train)\n",
    "temp = tfidfTransformer.fit_transform(temp)\n",
    "temp = nmf.fit_transform(temp)\n",
    "predictions = classifier.fit_predict(temp, y_train)\n",
    "\n",
    "## Get the results\n",
    "last_component       = classifier\n",
    "permutation_results  = last_component.mapping_permute_results\n",
    "best_mapping         = last_component.best_mapping\n",
    "best_accuracy_score  = last_component.best_accuracy_score\n",
    "remapped_predictions = last_component.remap_prediction_to_string(predictions) # Remap the predictions: integers -> strings\n",
    "\n",
    "## Print the results\n",
    "print( f\"Best mapping:        {best_mapping}\", end='\\n'*2 )\n",
    "print( f\"Best accuracy score: {best_accuracy_score}\", end='\\n'*2 )\n",
    "\n",
    "print( \"NMF Permute and Find best Mapping Results: \" )\n",
    "limitlessPandas(display)(permutation_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe396ad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T04:05:49.716952Z",
     "iopub.status.busy": "2024-05-01T04:05:49.716439Z",
     "iopub.status.idle": "2024-05-01T04:05:49.722948Z",
     "shell.execute_reply": "2024-05-01T04:05:49.721454Z"
    },
    "papermill": {
     "duration": 0.018881,
     "end_time": "2024-05-01T04:05:49.725918",
     "exception": false,
     "start_time": "2024-05-01T04:05:49.707037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## EXPLORE THIS LATER\n",
    "################################################################################\n",
    "\n",
    "# class SVC(SVC): \n",
    "#     def __init__(self, *args, **kwargs): \n",
    "#         super().__init__(*args, **kwargs) # Only need this if I want to init additional things in the subclass\n",
    "        \n",
    "#     def fit_predict(self, X, y=None):   # Override the method\n",
    "#         self.super().fit(X)\n",
    "#         return self.super().predict(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74d738e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T04:05:49.743719Z",
     "iopub.status.busy": "2024-05-01T04:05:49.743254Z",
     "iopub.status.idle": "2024-05-01T04:05:54.163840Z",
     "shell.execute_reply": "2024-05-01T04:05:54.162470Z"
    },
    "papermill": {
     "duration": 4.432693,
     "end_time": "2024-05-01T04:05:54.166755",
     "exception": false,
     "start_time": "2024-05-01T04:05:49.734062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.6928956834532374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "## Supervised Learning - Support Vector Classifier (SVC) - type of SVM\n",
    "################################################################################\n",
    "\n",
    "## Load the data and train/test split\n",
    "dataloader = Dataloader()\n",
    "X_train, X_test, y_train, y_test = dataloader.train_test_split(test_size=1, random_state=550)\n",
    "\n",
    "## IGNORE - Subsetting dataset for debugging\n",
    "# end=10\n",
    "# X_train = X_train[:end]\n",
    "# X_test = X_test[:end]\n",
    "# y_train = y_train[:end]\n",
    "# y_test = y_test[:end]\n",
    "\n",
    "## Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    #('lemmatizer', SpaCy_transformer()),  # Disabling lemmatizer - Too slow\n",
    "    ('countVectorizer', CountVectorizer()),    # has fit(), transform(), and fit_transform() - [type of transformer] - vectorizer\n",
    "    ('tfidfTransformer', TfidfTransformer()),   # has fit(), transform(), and fit_transform() - transformer\n",
    "    ('nmf', NMF(n_components=5)),  # has fit(), transform() and fit_transform() - transformer\n",
    "    ('svc', SVC()),\n",
    "])\n",
    "\n",
    "\n",
    "## Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)  # Training set\n",
    "\n",
    "## Get the results\n",
    "score = pipeline.score(X_train, y_train)\n",
    "\n",
    "## Print the results\n",
    "print( f\"Accuracy score: {score}\", end='\\n'*2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e63b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T04:05:54.183406Z",
     "iopub.status.busy": "2024-05-01T04:05:54.182899Z",
     "iopub.status.idle": "2024-05-01T04:15:17.029082Z",
     "shell.execute_reply": "2024-05-01T04:15:17.025974Z"
    },
    "papermill": {
     "duration": 562.866374,
     "end_time": "2024-05-01T04:15:17.040189",
     "exception": false,
     "start_time": "2024-05-01T04:05:54.173815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, error_score=&#x27;raise&#x27;,\n",
       "                   estimator=Pipeline(steps=[(&#x27;countVectorizer&#x27;,\n",
       "                                              CountVectorizer()),\n",
       "                                             (&#x27;tfidfTransformer&#x27;,\n",
       "                                              TfidfTransformer()),\n",
       "                                             (&#x27;nmf&#x27;, NMF(n_components=5)),\n",
       "                                             (&#x27;svc&#x27;, SVC())]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;countVectorizer__ngram_range&#x27;: [(1, 1),\n",
       "                                                                         (1, 2),\n",
       "                                                                         (1, 3),\n",
       "                                                                         (1, 4),\n",
       "                                                                         (1,\n",
       "                                                                          5)],\n",
       "                                        &#x27;countVectorizer__stop_words&#x27;: [&#x27;english&#x27;,\n",
       "                                                                        None],\n",
       "                                        &#x27;nmf__n_component...\n",
       "       30.6122449 , 31.63265306, 32.65306122, 33.67346939, 34.69387755,\n",
       "       35.71428571, 36.73469388, 37.75510204, 38.7755102 , 39.79591837,\n",
       "       40.81632653, 41.83673469, 42.85714286, 43.87755102, 44.89795918,\n",
       "       45.91836735, 46.93877551, 47.95918367, 48.97959184, 50.        ]),\n",
       "                                        &#x27;tfidfTransformer__norm&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                                        &#x27;tfidfTransformer__sublinear_tf&#x27;: [True,\n",
       "                                                                           False]},\n",
       "                   return_train_score=True, verbose=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, error_score=&#x27;raise&#x27;,\n",
       "                   estimator=Pipeline(steps=[(&#x27;countVectorizer&#x27;,\n",
       "                                              CountVectorizer()),\n",
       "                                             (&#x27;tfidfTransformer&#x27;,\n",
       "                                              TfidfTransformer()),\n",
       "                                             (&#x27;nmf&#x27;, NMF(n_components=5)),\n",
       "                                             (&#x27;svc&#x27;, SVC())]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;countVectorizer__ngram_range&#x27;: [(1, 1),\n",
       "                                                                         (1, 2),\n",
       "                                                                         (1, 3),\n",
       "                                                                         (1, 4),\n",
       "                                                                         (1,\n",
       "                                                                          5)],\n",
       "                                        &#x27;countVectorizer__stop_words&#x27;: [&#x27;english&#x27;,\n",
       "                                                                        None],\n",
       "                                        &#x27;nmf__n_component...\n",
       "       30.6122449 , 31.63265306, 32.65306122, 33.67346939, 34.69387755,\n",
       "       35.71428571, 36.73469388, 37.75510204, 38.7755102 , 39.79591837,\n",
       "       40.81632653, 41.83673469, 42.85714286, 43.87755102, 44.89795918,\n",
       "       45.91836735, 46.93877551, 47.95918367, 48.97959184, 50.        ]),\n",
       "                                        &#x27;tfidfTransformer__norm&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                                        &#x27;tfidfTransformer__sublinear_tf&#x27;: [True,\n",
       "                                                                           False]},\n",
       "                   return_train_score=True, verbose=4)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countVectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;tfidfTransformer&#x27;, TfidfTransformer()),\n",
       "                (&#x27;nmf&#x27;, NMF(n_components=5)), (&#x27;svc&#x27;, SVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(n_components=5)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "                   estimator=Pipeline(steps=[('countVectorizer',\n",
       "                                              CountVectorizer()),\n",
       "                                             ('tfidfTransformer',\n",
       "                                              TfidfTransformer()),\n",
       "                                             ('nmf', NMF(n_components=5)),\n",
       "                                             ('svc', SVC())]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'countVectorizer__ngram_range': [(1, 1),\n",
       "                                                                         (1, 2),\n",
       "                                                                         (1, 3),\n",
       "                                                                         (1, 4),\n",
       "                                                                         (1,\n",
       "                                                                          5)],\n",
       "                                        'countVectorizer__stop_words': ['english',\n",
       "                                                                        None],\n",
       "                                        'nmf__n_component...\n",
       "       30.6122449 , 31.63265306, 32.65306122, 33.67346939, 34.69387755,\n",
       "       35.71428571, 36.73469388, 37.75510204, 38.7755102 , 39.79591837,\n",
       "       40.81632653, 41.83673469, 42.85714286, 43.87755102, 44.89795918,\n",
       "       45.91836735, 46.93877551, 47.95918367, 48.97959184, 50.        ]),\n",
       "                                        'tfidfTransformer__norm': ['l1', 'l2'],\n",
       "                                        'tfidfTransformer__sublinear_tf': [True,\n",
       "                                                                           False]},\n",
       "                   return_train_score=True, verbose=4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"countVectorizer__stop_words\": ['english', None], \n",
    "    \"countVectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5)], \n",
    "    \"tfidfTransformer__norm\": ['l1', 'l2'], \n",
    "    \"tfidfTransformer__sublinear_tf\": [True, False], \n",
    "    \"nmf__n_components\": [5], #np.arange(5, 20, 5, dtype=int),\n",
    "    \"svc__C\": np.linspace(0, 50, 50), \n",
    "}\n",
    "\n",
    "searchCV = RandomizedSearchCV(\n",
    "    estimator=pipeline, \n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10, \n",
    "    cv=5,\n",
    "    verbose=4, \n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "searchCV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab068c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T04:15:17.058322Z",
     "iopub.status.busy": "2024-05-01T04:15:17.057809Z",
     "iopub.status.idle": "2024-05-01T04:15:17.108287Z",
     "shell.execute_reply": "2024-05-01T04:15:17.106613Z"
    },
    "papermill": {
     "duration": 0.063658,
     "end_time": "2024-05-01T04:15:17.111694",
     "exception": false,
     "start_time": "2024-05-01T04:15:17.048036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_tfidfTransformer__sublinear_tf</th>\n",
       "      <th>param_tfidfTransformer__norm</th>\n",
       "      <th>param_svc__C</th>\n",
       "      <th>param_nmf__n_components</th>\n",
       "      <th>param_countVectorizer__stop_words</th>\n",
       "      <th>param_countVectorizer__ngram_range</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.608279</td>\n",
       "      <td>0.097339</td>\n",
       "      <td>0.401493</td>\n",
       "      <td>0.027546</td>\n",
       "      <td>False</td>\n",
       "      <td>l2</td>\n",
       "      <td>16.326531</td>\n",
       "      <td>5</td>\n",
       "      <td>english</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691085</td>\n",
       "      <td>0.022852</td>\n",
       "      <td>3</td>\n",
       "      <td>0.695334</td>\n",
       "      <td>0.704890</td>\n",
       "      <td>0.704890</td>\n",
       "      <td>0.696459</td>\n",
       "      <td>0.710112</td>\n",
       "      <td>0.702337</td>\n",
       "      <td>0.005605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.517072</td>\n",
       "      <td>0.117545</td>\n",
       "      <td>0.402350</td>\n",
       "      <td>0.013763</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>6.122449</td>\n",
       "      <td>5</td>\n",
       "      <td>english</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705475</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703204</td>\n",
       "      <td>0.711074</td>\n",
       "      <td>0.712760</td>\n",
       "      <td>0.705453</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0.709756</td>\n",
       "      <td>0.004794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.926927</td>\n",
       "      <td>0.757961</td>\n",
       "      <td>1.238559</td>\n",
       "      <td>0.177019</td>\n",
       "      <td>False</td>\n",
       "      <td>l2</td>\n",
       "      <td>36.734694</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680744</td>\n",
       "      <td>0.022954</td>\n",
       "      <td>4</td>\n",
       "      <td>0.697021</td>\n",
       "      <td>0.712198</td>\n",
       "      <td>0.708263</td>\n",
       "      <td>0.699269</td>\n",
       "      <td>0.710674</td>\n",
       "      <td>0.705485</td>\n",
       "      <td>0.006164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.416993</td>\n",
       "      <td>2.641357</td>\n",
       "      <td>1.319584</td>\n",
       "      <td>0.111116</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>46.938776</td>\n",
       "      <td>5</td>\n",
       "      <td>english</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606986</td>\n",
       "      <td>0.046281</td>\n",
       "      <td>7</td>\n",
       "      <td>0.709949</td>\n",
       "      <td>0.717257</td>\n",
       "      <td>0.658797</td>\n",
       "      <td>0.709949</td>\n",
       "      <td>0.658989</td>\n",
       "      <td>0.690988</td>\n",
       "      <td>0.026341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.136922</td>\n",
       "      <td>1.929528</td>\n",
       "      <td>1.241136</td>\n",
       "      <td>0.067119</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>23.469388</td>\n",
       "      <td>5</td>\n",
       "      <td>english</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.059538</td>\n",
       "      <td>6</td>\n",
       "      <td>0.707701</td>\n",
       "      <td>0.714446</td>\n",
       "      <td>0.653176</td>\n",
       "      <td>0.707701</td>\n",
       "      <td>0.660112</td>\n",
       "      <td>0.688627</td>\n",
       "      <td>0.026322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.969335</td>\n",
       "      <td>0.797944</td>\n",
       "      <td>1.840210</td>\n",
       "      <td>0.158186</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>32.653061</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680301</td>\n",
       "      <td>0.023050</td>\n",
       "      <td>5</td>\n",
       "      <td>0.706015</td>\n",
       "      <td>0.711074</td>\n",
       "      <td>0.709387</td>\n",
       "      <td>0.708825</td>\n",
       "      <td>0.717416</td>\n",
       "      <td>0.710543</td>\n",
       "      <td>0.003803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.633237</td>\n",
       "      <td>0.747930</td>\n",
       "      <td>0.709599</td>\n",
       "      <td>0.021937</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>17.346939</td>\n",
       "      <td>5</td>\n",
       "      <td>english</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701881</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>2</td>\n",
       "      <td>0.707139</td>\n",
       "      <td>0.717257</td>\n",
       "      <td>0.713322</td>\n",
       "      <td>0.706015</td>\n",
       "      <td>0.714607</td>\n",
       "      <td>0.711668</td>\n",
       "      <td>0.004361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>85.349189</td>\n",
       "      <td>11.090499</td>\n",
       "      <td>2.610479</td>\n",
       "      <td>0.138058</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>43.877551</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>(1, 5)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251784</td>\n",
       "      <td>0.028345</td>\n",
       "      <td>10</td>\n",
       "      <td>0.387296</td>\n",
       "      <td>0.395166</td>\n",
       "      <td>0.395166</td>\n",
       "      <td>0.383924</td>\n",
       "      <td>0.414045</td>\n",
       "      <td>0.395119</td>\n",
       "      <td>0.010438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>61.225397</td>\n",
       "      <td>13.698220</td>\n",
       "      <td>1.627414</td>\n",
       "      <td>0.063302</td>\n",
       "      <td>True</td>\n",
       "      <td>l1</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459095</td>\n",
       "      <td>0.028540</td>\n",
       "      <td>9</td>\n",
       "      <td>0.618887</td>\n",
       "      <td>0.603148</td>\n",
       "      <td>0.563800</td>\n",
       "      <td>0.555368</td>\n",
       "      <td>0.628090</td>\n",
       "      <td>0.593859</td>\n",
       "      <td>0.029222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48.388607</td>\n",
       "      <td>8.319969</td>\n",
       "      <td>1.499802</td>\n",
       "      <td>0.194526</td>\n",
       "      <td>False</td>\n",
       "      <td>l1</td>\n",
       "      <td>26.530612</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513060</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>8</td>\n",
       "      <td>0.605958</td>\n",
       "      <td>0.617201</td>\n",
       "      <td>0.642496</td>\n",
       "      <td>0.647555</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.626237</td>\n",
       "      <td>0.015999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       2.608279      0.097339         0.401493        0.027546   \n",
       "1       2.517072      0.117545         0.402350        0.013763   \n",
       "2      24.926927      0.757961         1.238559        0.177019   \n",
       "3      41.416993      2.641357         1.319584        0.111116   \n",
       "4      40.136922      1.929528         1.241136        0.067119   \n",
       "5      42.969335      0.797944         1.840210        0.158186   \n",
       "6      12.633237      0.747930         0.709599        0.021937   \n",
       "7      85.349189     11.090499         2.610479        0.138058   \n",
       "8      61.225397     13.698220         1.627414        0.063302   \n",
       "9      48.388607      8.319969         1.499802        0.194526   \n",
       "\n",
       "  param_tfidfTransformer__sublinear_tf param_tfidfTransformer__norm  \\\n",
       "0                                False                           l2   \n",
       "1                                 True                           l1   \n",
       "2                                False                           l2   \n",
       "3                                 True                           l2   \n",
       "4                                 True                           l2   \n",
       "5                                 True                           l2   \n",
       "6                                 True                           l2   \n",
       "7                                 True                           l1   \n",
       "8                                 True                           l1   \n",
       "9                                False                           l1   \n",
       "\n",
       "  param_svc__C param_nmf__n_components param_countVectorizer__stop_words  \\\n",
       "0    16.326531                       5                           english   \n",
       "1     6.122449                       5                           english   \n",
       "2    36.734694                       5                              None   \n",
       "3    46.938776                       5                           english   \n",
       "4    23.469388                       5                           english   \n",
       "5    32.653061                       5                              None   \n",
       "6    17.346939                       5                           english   \n",
       "7    43.877551                       5                              None   \n",
       "8    35.714286                       5                              None   \n",
       "9    26.530612                       5                              None   \n",
       "\n",
       "  param_countVectorizer__ngram_range  ... mean_test_score  std_test_score  \\\n",
       "0                             (1, 1)  ...        0.691085        0.022852   \n",
       "1                             (1, 1)  ...        0.705475        0.016093   \n",
       "2                             (1, 2)  ...        0.680744        0.022954   \n",
       "3                             (1, 4)  ...        0.606986        0.046281   \n",
       "4                             (1, 4)  ...        0.636656        0.059538   \n",
       "5                             (1, 3)  ...        0.680301        0.023050   \n",
       "6                             (1, 2)  ...        0.701881        0.015437   \n",
       "7                             (1, 5)  ...        0.251784        0.028345   \n",
       "8                             (1, 3)  ...        0.459095        0.028540   \n",
       "9                             (1, 3)  ...        0.513060        0.029730   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                3            0.695334            0.704890   \n",
       "1                1            0.703204            0.711074   \n",
       "2                4            0.697021            0.712198   \n",
       "3                7            0.709949            0.717257   \n",
       "4                6            0.707701            0.714446   \n",
       "5                5            0.706015            0.711074   \n",
       "6                2            0.707139            0.717257   \n",
       "7               10            0.387296            0.395166   \n",
       "8                9            0.618887            0.603148   \n",
       "9                8            0.605958            0.617201   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.704890            0.696459            0.710112   \n",
       "1            0.712760            0.705453            0.716292   \n",
       "2            0.708263            0.699269            0.710674   \n",
       "3            0.658797            0.709949            0.658989   \n",
       "4            0.653176            0.707701            0.660112   \n",
       "5            0.709387            0.708825            0.717416   \n",
       "6            0.713322            0.706015            0.714607   \n",
       "7            0.395166            0.383924            0.414045   \n",
       "8            0.563800            0.555368            0.628090   \n",
       "9            0.642496            0.647555            0.617978   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0          0.702337         0.005605  \n",
       "1          0.709756         0.004794  \n",
       "2          0.705485         0.006164  \n",
       "3          0.690988         0.026341  \n",
       "4          0.688627         0.026322  \n",
       "5          0.710543         0.003803  \n",
       "6          0.711668         0.004361  \n",
       "7          0.395119         0.010438  \n",
       "8          0.593859         0.029222  \n",
       "9          0.626237         0.015999  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(searchCV.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359ca8f",
   "metadata": {
    "papermill": {
     "duration": 0.008111,
     "end_time": "2024-05-01T04:15:17.128336",
     "exception": false,
     "start_time": "2024-05-01T04:15:17.120225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 324297,
     "sourceId": 13351,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 594.288007,
   "end_time": "2024-05-01T04:15:19.764543",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-01T04:05:25.476536",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
