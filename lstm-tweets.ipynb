{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Predicting Disaster Tweets with a Simple LSTM Model\n","Anthony Lee 2024-11-26\n","\n","[GitHub archive](https://github.com/anthropikos/kaggle_kernels/blob/40d7c483c8f0d57a5b4708e9a126f428c79cdda6/lstm-tweets.ipynb)\n","\n","[Kaggle link](https://www.kaggle.com/code/anthonyylee/lstm-tweets)\n","\n","## Abstract\n","This notebook documents a simple LSTM model created to predict whether each tweet from a dataset pertains to a disaster. Instead of a bag-of-words approach, the LSTM model retains memory of what it was trained on and thus can learn from the sequences of words. Each tweet is read from a CSV and then tokenized, lemmatized, vectorized using SpaCy's `en_core_web_lg` pipeline and then used to train the LSTM model built in PyTorch. The training is unbatched and CPU based. The model takes approximately 7.5 minutes to train on a quad-core Intel Xeon 2.20Ghz CPU, and was used to inference on a test dataset achieving an F1 score of 0.803. Further, the notebook discusses some of the learnings from the development of the model and presents some suggestions on future improvements.\n","\n","## Methods\n","### Train / Validation Split\n","For overfitting detection, the training dataset was split by 90/10 with 90% of the data being training data and the remaining 10% being held out for validation. The split stratifies based on the training data's target ensuring that the proportion of diaster to non-disaster is equal between the training set and valdiation set.\n","\n","### NLP Pipeline\n","SpaCy was used for the text processing step. Using the tools and pretrained model in the `en_core_web_lg` pipeline, each tweet is tokenized, lemmatized, and vectorized for the deep learning model's consumption. The decision to include a lemmatization step is to reduce the domain space of each tweet as words retain similar meanings when reduced to its lemma form. The `en_core_web_lg` pipeline is approximatley 382MB with 685,000 keys and 343,000 unique vectors of 300 dimensions. The output of the pipeline is thus a two dimensional array of N x D where N is the sequence legnth of tweet, and D is the dimension of the token vector (300).\n","\n","### LSTM Architecture in PyTorch\n","The LSTM model is constructed using PyTorch and consists of one LSTM layer, one fully-connected (FC) layer, and a sigmoid activation function. The LSTM layer accepts arbitrary sequence of 300 dimension vectors with an output dimension of ten. The FC layer accepts input dimension of ten and outputs a dimension one scalar. Finally, the scalar is passed through a Sigmoid function to map into the probability space indicating the probability of the tweet pertaining to a disaster.\n","\n","A Mean-Square-Error (MSE) loss was used as the criterion and Adaptive Moment Estimation (Adam) (Kingma & Ba, 2017) is used in the LSTM model.\n","\n","\n","## Discussion\n","### CPU vs GPU\n","To utilize the acceleration provided by a GPU, the data should be batched to allow for parallelized processing of multiple data points (i.e., tweets). Unlike a convolutional model often used in computer vision, the LSTM is sequential model and cannot be meaningfully parallelized for a single data point. I opted out from using the GPU because 1) SpaCy `en_core_web_lg` pipeline is optimized for CPU, 2) copying data between CPU and GPU adds too much overhead for how small this dataset is, and 3) batching the tweets proves to be a challenge.\n","\n","### Batching vs Unbatching\n","To utilize GPUs, batching had to be done with the data. However, padding had to be added because of the varying lengths of each tweet. I chose to pad with a vector of zeros which maps to a space character. When padding the dataset with space characters, the model fails to understand the spaces as null and attempts to interpret the spaces resulting in the model parameters being zeroed out during the backpropagation step and constantly inferences to a value of zero thus losses its inferencing power.\n","\n","### Hyperparameter tuning the learning rate\n","The Adam optimizer has multiple hyperparameters that can be adjusted and the main one is the learning rate. Without systematic hyperparameter tuning I tested a comically large learning rate of 300 and and small learning rate or 0.001. Due to the lack of batching, the parameters are updated for each data point used for training and thus a large learning rate constantly overshoots the optimal parameter value. Instead, a small learning rate is shown to perform much better and is sufficient enough that signs of overfitting starts showing after one or two epoch(s).\n","\n","![Train Loss vs Validation Loss](https://www.kaggleusercontent.com/kf/209127182/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..7RGNCq2PeJo-FZ6mmDNNaw.2p09SjEiaYSXXpWWOt3ECZ67mLuL-DN98x-jFIRiNrP8__SvwzHQZuh5P6k5kEYl3xK32WakvgjiA3nf-jGO3HT6n9KINmAxqn5KiMakeAttzyfUWC_w_gY8b968ZL2B6sBrA5wpote0ehGa0p5Kmte0_kPuEY7MVL0fpZ7pZwLoKqXwTX7vjpP-DlLGI3pWWHaxbKhwnIuJAg02BYu-2C04TIERG9i5TBvoYTCyA_gcehPeBc_NjrQIJQA18O_8UQ9oy9DcQwrDq5mrfNQOyDoeGbyxcSMNKLfJU-3T-W_DofiUHFqGAiWoVG1IbyLNAP4hth5PwVN0ilx_T-m_CX2cbbLuTggAvgUwfuYskTojP5KvfMi-asoozN-6zQen61ah29en5Y8OWUL3Jt52fW0h6hs7TINndcZfYx6vxdWf4h49CATELSA_tGiKxM89vsE7ZJ2oNc9Vn__AtkkxIrm4jRZfMEJso9YXcVIBcZBhB8xFxMNnFDhhUCEPYMce3DHgZXUW8eEG-yi4XpGEuVEt5mo1FWsSXbEYVuBqQPicmUX4ZbpjVISapvsQC__5tnYC6kd5j9cSB7R5E_HJ8wx5Yykbkf2ZcBCFnfB19KjAcsfkSourqysIVkNW1kKi.zCL0r__229ksbNBmi7e9sQ/__results___files/__results___7_1.png)\n","\n","## Conclusion\n","In conclusion, the LSTM model made parallelization more challenging and considering the overhead introduced when training with GPU, training and inferencing with CPU was the better approach. Even though padding the varying lengths of tweets proves to be challenging, there are options to still be batch train the model such as early halting the training when encountering a token of all zeros. Using the Adam optimizer and a small learning rate, the model is able to achieve F1 score of 0.803 with a training time of aprpoximatly 7.5 minutes. To push the model performance even further, a few suggested improvement is proposed below. However, this simple model shows the value that a model with memory is capable of extracting further information that a simpler model such as bag-of-words is not able to achieve. \n","\n","## Future Improvements\n","- Explore other vectorization methods\n","    - The pipeline model used is 382MB and takes some time to download/load. For a more portable inteligent device such as a IoT device, the storage requirement could make this model unfeasible. Thus, it would worth looking into utilizing SpaCy's other pretrained pipelines or another encoding method.\n","- Explore the use of transformers\n","    - Transformers are shown tremendous progress in the field of NLP and this is a valuable avenue to explore and compare performance with this simple LSTM model. Additionally, various pretrained transformer models have been published and shared freely in an open-source manner. To be able to further train these published model could help fine tune the performance of these published models.\n","- Explore how to batch train and utilize GPU\n","    - To be able to batch these varying lengths of tweets have proved challenging and could lead to errors if proper precautions were not considered. One suggested approach is to pad all tweets with a specialized sentinel vector and terminate the LSTM sequential training of each token when the sentinel is encountered.\n","- Explore how to deploy the trained model in an application that can be used to demo\n","    - Publish the model using some sort of interactive applet such that the model can be tested with other texts. \n","- Improve data cleaning and extraction\n","    - In this notebook, the NLP processing was essentially offloaded to the SpaCy library and their methodologies. Certain special characters remained in the the tweets and was vectorized accordingly. It would be valuable to create an alternative dataset of which the special characters such as emojis or emoticons were removed and train the LSTM model on these data. Through this comparison we could be able to hypothesize the amount of information informing whether the tweet pertains to a disaster is provided by these non-traditional characters.\n","- Systematic tune the hyperparamters\n","    - Use a tool such as RayTune to systematically tune the hyperparameter values and consider the impact of learning rate to the outcome of the model's inference power.\n","    - Also systematically tune and discover whether additional LSTM layers or LSTM hidden output dimensions could improve the model's performance.\n","- Permute the order of the training dataset\n","    - The order of the training dataset is static in this model and because of LSTM's memory, the order of which each training data is introduce could impact the outcome of model's parameters. Permute the order of the dataset and test if the model retains its inference power.\n","- Test removing lemmatizer\n","    - Considering that these are tweets thus proper English grammar may be lacking; there may be value in training on non-lemmatized tweets.\n","- Test training with increased LSTM layers and hidden output dimensions\n","\n","## References\n","Kingma, D. P., & Ba, J. (2017). Adam: A Method for Stochastic Optimization (No. arXiv:1412.6980). arXiv. https://doi.org/10.48550/arXiv.1412.6980\n"]},{"cell_type":"markdown","metadata":{},"source":["## Setting up the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-26T22:36:17.236032Z","iopub.status.busy":"2024-11-26T22:36:17.234843Z","iopub.status.idle":"2024-11-26T22:36:41.641321Z","shell.execute_reply":"2024-11-26T22:36:41.640144Z","shell.execute_reply.started":"2024-11-26T22:36:17.235970Z"},"trusted":true},"outputs":[],"source":["# Install the needed langauge package\n","!python -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-26T22:36:41.645109Z","iopub.status.busy":"2024-11-26T22:36:41.644304Z","iopub.status.idle":"2024-11-26T22:36:44.985950Z","shell.execute_reply":"2024-11-26T22:36:44.984324Z","shell.execute_reply.started":"2024-11-26T22:36:41.645022Z"},"trusted":true},"outputs":[],"source":["import gc\n","from typing import List, Union ,Iterable, Tuple, Dict\n","from multiprocessing import set_start_method, cpu_count\n","from pathlib import Path\n","import cProfile, pstats\n","from collections import namedtuple\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import spacy\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","\n","\n","def read_in_csv(data_dir_path:Path=None) -> Tuple:\n","    \"\"\"Read in the dataset CSVs and return a tuple of dataframes.\"\"\"\n","    \n","    OutputTuple = namedtuple(\"read_in_csv\", [\"train\", \"test\", \"submission_example\"])\n","    \n","    if data_dir_path is None: \n","        data_dir_path = Path(\"/kaggle/input\")\n","    \n","    ## Read in the data\n","    df_train = pd.read_csv(data_dir_path / Path(\"nlp-getting-started/train.csv\"))\n","    df_test = pd.read_csv(data_dir_path / Path(\"nlp-getting-started/test.csv\"))\n","    df_sample_submission = pd.read_csv(data_dir_path / Path(\"nlp-getting-started/sample_submission.csv\"))\n","\n","    return OutputTuple(train=df_train, test=df_test, submission_example=df_sample_submission)\n","\n","\n","def train_validation_split(df_train:pd.DataFrame, validation_percentage:float=None) -> Tuple:\n","    \"\"\"Train/Validation split the train dataframe.\"\"\"\n","    OutputTuple = namedtuple(\"train_validation_split\", [\"train_datas\", \"validation_datas\", \"train_targets\", \"validation_targets\"])\n","\n","    if validation_percentage is None: \n","        validation_percentage = 0.1\n","        \n","    train_percentage = 1 - validation_percentage    \n","    \n","    ## Simple Train/Validate split\n","    train_datas, validation_datas, train_targets, validation_targets = train_test_split(\n","        df_train.text.to_list(), \n","        df_train.target.to_list(),\n","        train_size = train_percentage, \n","        random_state = 7,  # For consistency\n","        stratify = df_train.target.to_list(),\n","    )\n","\n","    return OutputTuple(train_datas=train_datas, \n","                       validation_datas=validation_datas, \n","                       train_targets=train_targets, \n","                       validation_targets=validation_targets\n","                      )\n","\n","\n","def text_to_vector(documents:Iterable, n_process=None) -> List:\n","    \"\"\"Iterate through a list of documents and returns a list of vectorized documents.\n","    \n","    Uses spaCy's en_core_web_log model to transform each tweet into a 2D ndarray\n","    of floats. Each ndarray is the word_count by 300 where 300 is the vector length\n","    of each token used in the spaCy model.\n","    \"\"\"\n","    if not isinstance(documents, Iterable):\n","        raise TypeError(\"`documents` has to be a list of strings.\")\n","        \n","    nlp = spacy.load(\"en_core_web_lg\")\n","    \n","    vec_length = 300  # Token vector length in SpaCy\n","    \n","    if n_process is None: \n","        n_process = cpu_count()\n","        \n","    docs = nlp.pipe(texts=documents, n_process=n_process, batch_size=50)\n","    \n","    holder_all_tweets = []\n","    \n","    for doc in docs:\n","        tweet_length = len(doc)\n","        doc_ndarray = np.zeros(shape=(tweet_length, vec_length), dtype=np.float64)\n","\n","        for idx, token in enumerate(doc):\n","            doc_ndarray[idx, :] = token.vector\n","        holder_all_tweets.append(doc_ndarray)\n","        \n","    return holder_all_tweets\n","\n","\n","class DisasterTweetDataset(Dataset):\n","    def __init__(self, vectorized_tweets:Iterable, targets:Iterable) -> None:\n","        self.vectorized_tweets = vectorized_tweets\n","        self.targets = targets\n","        \n","        self.__data_validation()\n","\n","    def __len__(self) -> int: \n","        return len(self.targets)\n","    \n","    def __getitem__(self, idx:int) -> Tuple[List, List]:\n","        ReturnedResult = namedtuple(\"disaster_tweet\", [\"target\", \"vectorized_tweet\"])\n","\n","        target = self.targets[idx]\n","        vectorized_tweet = self.vectorized_tweets[idx]\n","        \n","        return ReturnedResult(target=target, vectorized_tweet=vectorized_tweet)\n","\n","    def __data_validation(self) -> None:\n","        if len(self.vectorized_tweets) != len(self.targets): \n","            raise ValueError(f\"The data counts do NOT match, got {len(self.vectorized_tweets)} and {len(self.targets)}\")\n","\n","\n","def checkpoint_save(model, optimizer, epoch, training_loss, validation_loss, dir_path=None):\n","    \"\"\"Save the model checkpoint to current working directory.\"\"\"\n","    \n","    #from datetime import datetime, timezone\n","    #timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M_%S\")\n","    #filename = f\"torch_checkpoint_epoch_{epoch}_{timestamp}.checkpoint\"\n","    filename = f\"checkpoint_epoch_{epoch}.checkpoint\"\n","    \n","    if dir_path is None: \n","        dir_path = Path.cwd()\n","    else: \n","        dir_path = Path(dir_path)\n","    \n","    dict_to_save = {\n","        \"epoch\": epoch,\n","        \"training_loss\": training_loss,\n","        \"validation_loss\": validation_loss,\n","        \n","        \"model_class\": model.__class__,\n","        #\"model_class_name\": model.__class__.__name__,\n","        \"model_state_dict\": model.state_dict(),\n","        \n","        \"optimizer_class\": optimizer.__class__,\n","        #\"optimizer_class_name\": optimizer.__class__.__name__,\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","    }\n","    \n","    torch.save(dict_to_save, dir_path/filename)\n","    \n","    return dict_to_save\n","    \n","\n","def checkpoint_load(file_path):\n","    \"\"\"Instantiates and loads the state of the model and optimizer from the checkpoint.\"\"\"\n","\n","    ## TODO: Need to fix this loading function such that when LSTM model have different \n","    ##      architecture it would still work. For example, more than 1 LSTM layers and/or more\n","    ##      than 10 hidden output dimensions. Currently assumes default architecture.\n","    \n","    CheckpointLoad = namedtuple(\"checkpoint_load\", [\"model\", \"optimizer\", \"training_loss\", \"validation_loss\"])\n","    file_path = Path(file_path)\n","    checkpoint = torch.load(file_path, weights_only=False)\n","\n","    # Create and load model state dict\n","    model = checkpoint[\"model_class\"]()  # Instantiate using the class name\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","\n","    # Create and load optimizer state dict\n","    optimizer = checkpoint[\"optimizer_class\"](model.parameters())  # Instantiate using the class name\n","    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","\n","    # Load loss information\n","    training_loss = checkpoint[\"training_loss\"]\n","    validation_loss = checkpoint[\"validation_loss\"]\n","\n","    return CheckpointLoad(model=model, optimizer=optimizer, training_loss=training_loss, validation_loss=validation_loss)\n","\n","    \n","def plot_train_validation_loss(avg_training_loss:np.ndarray, avg_validation_loss:np.ndarray) -> mpl.axes.Axes:\n","    assert len(avg_training_loss) == len(avg_validation_loss), f\"Training loss and validation loss arrays should have the same length, got {len(avg_training_loss)} and {len(avg_validation_loss)}\"\n","    \n","    fig, ax = plt.subplots()\n","    marker_size = 10\n","\n","    ax.set_title(\"Avg training and validation loss for each epoch\")\n","    ax.set_ylabel(\"Avg [training | validation] loss\")\n","    ax.set_xlabel(\"Epoch\")\n","\n","    num_of_epochs = len(avg_training_loss)\n","    \n","    ax.scatter(range(num_of_epochs), avg_training_loss, color=\"C1\", s=marker_size, label=\"avg training loss\")\n","    ax.scatter(range(num_of_epochs), avg_validation_loss, color=\"C2\", s=marker_size ,label=\"avg validation loss\")\n","    \n","    ax.plot(avg_training_loss, \"--\", alpha=0.3, color=\"C1\")\n","    ax.plot(avg_validation_loss, \"--\", alpha=0.3, color=\"C2\")\n","\n","    ax.legend(loc=\"upper right\")\n","    ax.grid(visible=True, which=\"both\", axis=\"both\", alpha=0.2)\n","    return ax\n","\n","\n","def predict_test_data_for_submission(model:torch.nn.Module, df_test:pd.DataFrame, save:bool=None) -> List:\n","    \"\"\"Convenience function to predict for submission.\"\"\"\n","    if save is None:\n","        save = False\n","\n","    test_datas_vectorized = text_to_vector(df_test.text.to_list())\n","    \n","    model.train(False)\n","    holder = []\n","    \n","    for item in test_datas_vectorized: \n","        prediction = model(torch.tensor(item)).detach()\n","    \n","        # Convert probability to categorical label\n","        if prediction > 0.5: \n","            prediction = 1\n","        else:\n","            prediction = 0\n","        \n","        holder.append(prediction)\n","\n","    if save is True:\n","        submission = pd.DataFrame({\"id\": df_test.id, \"target\": holder})\n","        submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n","\n","    return holder\n","\n","\n","class SimpleLSTM(torch.nn.Module):\n","    \n","    def __init__(self, input_size:int=None, lstm_num_layers:int=None, lstm_hidden_size:int=None) -> None:\n","        \"\"\"Simple LSTM model.\n","        \n","        Structure:\n","            LSTM-Layer(s) > Dense-Layer > Sigmoid activation function\n","\n","        - Unable to to train in batches as each tweet (or document) has varied length.\n","        - Batch normalization is not really needed because outputs have a sigmoid activation function.\n","        \"\"\"\n","        super().__init__()\n","        \n","        # Defaults\n","        if input_size is None: \n","            input_size = 300  # SpaCy vector size are 300\n","        if lstm_num_layers is None: \n","            lstm_num_layers = 1\n","        if lstm_hidden_size is None: \n","            lstm_hidden_size = 10  # LSTM output to have 10 dimensions\n","        target_output_size = 1\n","        \n","        # Variables\n","        self.input_size = int(input_size)\n","        self.lstm_num_layers = int(lstm_num_layers)\n","        self.lstm_hidden_size = int(lstm_hidden_size)\n","        self.target_output_size = int(target_output_size)\n","        self.dtype = torch.float64\n","        \n","        # Layers\n","        self.layer_lstm = torch.nn.LSTM(\n","            input_size=self.input_size, \n","            hidden_size=self.lstm_hidden_size,\n","            num_layers=self.lstm_num_layers,\n","            bias=True,\n","            batch_first=True,  # Batch first is more nature, but hidden and cell state outputs are not batch first (see PyTorch documentation)\n","            dropout=0.0,\n","            bidirectional=False,\n","            dtype=self.dtype,\n","        )\n","        \n","        # Fully connected layer\n","        self.layer_fc = torch.nn.Linear(\n","            in_features=self.lstm_hidden_size,\n","            out_features=self.target_output_size,\n","            bias=True,\n","            dtype=self.dtype,\n","        )\n","        \n","        # Final sigmoid layer\n","        self.layer_sigmoid = torch.nn.Sigmoid()  # transforms to probability space\n","        self.to(dtype=torch.float64)  # self.double() also works\n","        \n","        return\n","\n","    \n","    def forward(self, input_data:torch.Tensor) -> torch.Tensor:\n","        \"\"\"Forward calculation, call the module instance instead.\n","        \n","        Even though this method is defined, one should call the module instance instead to make\n","        sure that all the registered hooks are taken care of.\n","        Source: https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward\n","        \"\"\"\n","        \n","        # LSTM layer\n","        # - last_layer_output: the last layer output for each sequence of the input sequence (tokens of a sentence)\n","        # - Output vs h_n: The former only has output for the last LSTM layer, whereas the latter has output for all LSTM layers.\n","        last_layer_output, (h_n, c_n) = self.layer_lstm(input_data)\n","        if last_layer_output.dim() == 3: \n","            last_layer_output = last_layer_output[:, -1, :]  # All batches; Last sequence output; hidden size\n","        elif last_layer_output.dim() == 2:\n","            last_layer_output = last_layer_output[-1, :]  # When no batches - Last seq output; hidden size\n","        else:\n","            raise ValueError(f\"Output of LSTM layer expected to be either 3 or 2 dimensions (got {last_layer_output.dim()} dimensions.)\")\n","\n","        # Dense fully connected layer\n","        output = torch.squeeze(last_layer_output)     \n","        output = self.layer_fc(output)\n","        \n","        # Sigmoid activation function\n","        output = self.layer_sigmoid(output)\n","        \n","        return output\n","\n","\n","def validation_loss(model:torch.nn.Module, criterion:torch.nn.modules.loss._Loss, validation_dataset:torch.utils.data.Dataset):\n","    \"\"\"Calculate validation loss and return last loss and running loss of the model.\"\"\"\n","    \n","    BatchedValidationLoss = namedtuple(\"batched_validation_loss\", [\"last_loss\", \"running_loss\"])\n","    \n","    last_validation_loss = 0\n","    running_validation_loss = 0\n","\n","    model.train(False)  # Eval mode\n","\n","    with torch.no_grad():\n","        for idx_data, (validation_target, validation_data) in enumerate(validation_dataset):\n","\n","            # Inference\n","            prediction = model(torch.tensor(validation_data))\n","            \n","            # Calculate loss\n","            loss = criterion(prediction, torch.tensor(validation_target).double())\n","            \n","            # Keep track of the loss\n","            last_validation_loss = loss.detach()  # Solves memory leak - Or use loss.item()\n","            running_validation_loss += last_validation_loss\n","            \n","    return BatchedValidationLoss(last_loss=last_validation_loss, running_loss=running_validation_loss)\n","\n","\n","def train_SimpleLSTM(model:torch.nn.Module, optimizer:torch.optim.Optimizer, criterion:torch.nn.modules.loss._Loss, \n","                    train_dataset:torch.utils.data.Dataset):\n","    \"\"\"Model training function, and returns the last loss and running loss as a tuple.\"\"\"\n","    \n","    BatchedTrainLoss = namedtuple(\"batched_train_loss\", [\"last_loss\", \"running_loss\"])\n","    last_train_loss = 0\n","    running_train_loss = 0\n","\n","    model.train(True)  # Training mode\n","    \n","    # Training loop\n","    for idx_data, (train_target, train_data) in enumerate(tqdm(train_dataset, desc=\"    Training...\", unit=\"Tweet\", miniters=100)):\n","        #if idx_batch == 5: break  # DEBUG\n","\n","        # Forward prop\n","        model.zero_grad()  # Zero out the graident\n","        optimizer.zero_grad()\n","        prediction = model(torch.tensor(train_data))\n","        \n","        # Calculate loss\n","        loss = criterion(prediction, torch.tensor(train_target).double())\n","        \n","        # Backward prop\n","        loss.backward()  # Calculate gradients after the loss is aggregated with the reduction strategy\n","        optimizer.step() # Update parameter with gradients\n","        \n","        # Keep track of loss\n","        last_train_loss = loss.detach()  # Solves memory leak - Or loss.item()\n","        running_train_loss += last_train_loss\n","        \n","    return BatchedTrainLoss(last_loss=last_train_loss, running_loss=running_train_loss)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data Processing\n","The data are read in from CSV, train-validation splitted, vectorized, and then wrapped as PyTorch Datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-26T22:36:44.988663Z","iopub.status.busy":"2024-11-26T22:36:44.987966Z","iopub.status.idle":"2024-11-26T22:37:29.244004Z","shell.execute_reply":"2024-11-26T22:37:29.242518Z","shell.execute_reply.started":"2024-11-26T22:36:44.988617Z"},"trusted":true},"outputs":[],"source":["## Data Processing Steps ##\n","\n","# (1) Read in the CSVs\n","df_train, df_test, df_sample_submission = read_in_csv()\n","\n","# (2) Train / Validation split\n","train_datas, validation_datas, train_targets, validation_targets = train_validation_split(\n","    df_train=df_train, \n","    validation_percentage=0.1\n",")\n","\n","# (3) Vectorize the text datas so can be processed by the model\n","train_datas_vectorized = text_to_vector(train_datas)\n","validation_datas_vectorized = text_to_vector(validation_datas)\n","test_datas_vectorized = text_to_vector(df_test.text.to_list())\n","\n","# (4) Turn them into dataset for convenience\n","disaster_tweet_dataset = DisasterTweetDataset(train_datas_vectorized, train_targets)\n","disaster_tweet_dataset_validation = DisasterTweetDataset(validation_datas_vectorized, validation_targets)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Understanding the Training dataset\n","The training dataset includes id, keyword, location, text, and target information. However, the keyword and location columns are sparsely populated and contains a lot of characters encoded in some other encoding methods that do not seem to contain valuable information for what we are seeking here."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-26T22:41:09.157986Z","iopub.status.busy":"2024-11-26T22:41:09.157649Z","iopub.status.idle":"2024-11-26T22:41:09.170291Z","shell.execute_reply":"2024-11-26T22:41:09.168968Z","shell.execute_reply.started":"2024-11-26T22:41:09.157957Z"},"trusted":true},"outputs":[],"source":["df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-26T22:40:29.964519Z","iopub.status.busy":"2024-11-26T22:40:29.964085Z","iopub.status.idle":"2024-11-26T22:40:29.991714Z","shell.execute_reply":"2024-11-26T22:40:29.990478Z","shell.execute_reply.started":"2024-11-26T22:40:29.964476Z"},"trusted":true},"outputs":[],"source":["## Pivot table for location and target\n","df_train.pivot_table(values='id', columns='target', index='location', aggfunc='count', dropna=False, fill_value=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-26T22:46:06.726373Z","iopub.status.busy":"2024-11-26T22:46:06.725998Z","iopub.status.idle":"2024-11-26T22:46:06.748688Z","shell.execute_reply":"2024-11-26T22:46:06.747262Z","shell.execute_reply.started":"2024-11-26T22:46:06.726341Z"},"trusted":true},"outputs":[],"source":["## Pivot keyword vs target\n","df_train.pivot_table(values='id', columns='target', index='keyword', aggfunc='count', dropna=False, fill_value=0)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset distribution\n","There are more non-disaster tweets in this dataset than disaster ones, hence the importance of stratifying the two targets (disaster and non-disaster) when splitting them into the train/validation sets."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-26T22:43:43.676676Z","iopub.status.busy":"2024-11-26T22:43:43.676239Z","iopub.status.idle":"2024-11-26T22:43:43.799470Z","shell.execute_reply":"2024-11-26T22:43:43.797946Z","shell.execute_reply.started":"2024-11-26T22:43:43.676638Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["## Plotting target balance - disaster vs non-disaster\n","def pie_chart_label(pct, all_data): \n","    output = pct/100*np.sum(all_data)\n","    return f\"{output}\"\n","    \n","fig, ax = plt.subplots()\n","target_label_mapping = {0: \"Non-disaster\", 1:\"Disaster\"}\n","series_target_counts = df_train.target.value_counts()\n","\n","ax.pie( \n","    x=series_target_counts.values, \n","    labels=list(map(lambda label: target_label_mapping[label], \n","                    series_target_counts.index.values)), \n","    autopct=lambda pct: f\"{round(pct, 3)}%\",\n",")\n","\n","ax.set_title(\"Tweet Target distribution\")\n","plt.show(fig)"]},{"cell_type":"markdown","metadata":{},"source":["## Train the Model\n","Below we train the model with the parameter set to 10 epochs and a learning rate of 0.001."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-23T05:38:33.192985Z","iopub.status.busy":"2024-11-23T05:38:33.192629Z","iopub.status.idle":"2024-11-23T05:45:32.403876Z","shell.execute_reply":"2024-11-23T05:45:32.402726Z","shell.execute_reply.started":"2024-11-23T05:38:33.192952Z"},"trusted":true},"outputs":[],"source":["## Training the model!\n","\n","# Some parameters\n","num_of_epochs = 10\n","learning_rate = 0.001\n","\n","# Instantiating the model, optimizer, and criterion\n","lstm = SimpleLSTM()\n","optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n","criterion = torch.nn.MSELoss(reduction=\"none\")  # Non-batched, thus don't need reduction strategy\n","\n","holder_avg_train_loss = np.zeros(shape=num_of_epochs, dtype=np.float64)\n","holder_avg_validation_loss = np.zeros(shape=num_of_epochs, dtype=np.float64)\n","\n","# Epoch training loop\n","for idx_epoch in tqdm(range(num_of_epochs), desc=\"Training epoch...\", unit=\"Epoch\"):\n","    train_result = train_SimpleLSTM(lstm, optimizer, criterion, disaster_tweet_dataset)\n","    validation_result = validation_loss(lstm, criterion, disaster_tweet_dataset_validation)\n","\n","    # Calculate avg train loss and validation loss for this epoch\n","    running_train_loss = train_result.running_loss\n","    running_validation_loss = validation_result.running_loss\n","\n","    avg_train_loss = running_train_loss / len(disaster_tweet_dataset)\n","    avg_validation_loss = running_validation_loss / len(disaster_tweet_dataset_validation)\n","\n","    # Add to holder\n","    holder_avg_train_loss[idx_epoch] = avg_train_loss\n","    holder_avg_validation_loss[idx_epoch] = avg_validation_loss\n","\n","    # Checkpoint save\n","    checkpoint_save(lstm, optimizer, epoch=idx_epoch, \n","                    training_loss=avg_train_loss, \n","                    validation_loss=avg_validation_loss\n","                   )\n","\n","# Plot results\n","ax = plot_train_validation_loss(holder_avg_train_loss, holder_avg_validation_loss)\n","fig = ax.get_figure()\n","fig.savefig(\"/kaggle/working/avg_train_validation_loss_trend.svg\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load model snapshot and inference test dataset for competition"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-23T05:45:32.405922Z","iopub.status.busy":"2024-11-23T05:45:32.405345Z","iopub.status.idle":"2024-11-23T05:46:07.674984Z","shell.execute_reply":"2024-11-23T05:46:07.673496Z","shell.execute_reply.started":"2024-11-23T05:45:32.405885Z"},"trusted":true},"outputs":[],"source":["# Submit to competition\n","checkpoint = checkpoint_load(\"/kaggle/working/checkpoint_epoch_3.checkpoint\")  # Because after the 3rd epoch, shows overfitting\n","lstm = checkpoint.model\n","prediction = predict_test_data_for_submission(model=lstm, df_test=df_test, save=True)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":869809,"sourceId":17777,"sourceType":"competition"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
