{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "temp = nn.Conv2d(3, 5, 4)\n",
    "\n",
    "# pprint.pp(list(vars(temp).keys()))\n",
    "print(temp.in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7195,  1.7468, -0.6563,  2.1015, -0.8986, -0.6573, -0.9700,  0.5803,\n",
       "          0.2117, -0.2055],\n",
       "        [-0.2113, -1.0119, -0.3463, -0.2057,  0.6806, -0.6585, -0.9805,  0.9881,\n",
       "          0.6095,  1.5342],\n",
       "        [ 0.4946, -0.0395, -1.0333,  0.3965, -0.7368, -0.7466, -0.7460, -1.9278,\n",
       "          0.5940,  0.0713],\n",
       "        [-0.1568, -0.9164, -1.4041, -0.4023,  0.2728,  0.1468,  0.1836,  0.4164,\n",
       "         -0.6360,  0.0438],\n",
       "        [ 1.4649, -0.3185, -1.8801,  0.5836,  0.5632, -1.0180,  0.0981,  0.9947,\n",
       "         -2.0018, -0.5817],\n",
       "        [ 0.0411, -0.2180,  0.6279,  1.9570,  0.7057, -0.5234, -1.1305, -0.9255,\n",
       "         -1.2302,  0.5268],\n",
       "        [-0.4607, -0.9317, -0.5945,  0.5654, -0.0129, -1.5273, -0.0750,  0.3663,\n",
       "          1.0091,  1.5313],\n",
       "        [ 1.5763, -1.0368, -0.4461,  0.4914,  0.0549,  1.9135,  0.8207, -1.5554,\n",
       "          0.5412,  0.2240],\n",
       "        [-0.3940, -0.4267, -0.4693,  1.1137, -0.8119,  0.3434, -0.5579,  0.2417,\n",
       "         -0.5114, -1.4529],\n",
       "        [ 1.1832, -0.2157, -1.6464,  1.8412, -1.0069,  0.9888, -0.0147,  1.4664,\n",
       "          1.7171,  0.7019]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "dropout_layer = nn.Dropout(p=0.8)\n",
    "input = torch.randn(10, 10)\n",
    "output = dropout_layer(input)\n",
    "\n",
    "output\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "IN_layer_3 = nn.InstanceNorm2d(num_features=3)\n",
    "IN_layer_4 = nn.InstanceNorm2d(num_features=4)\n",
    "input = torch.randint(low=0, high=255, size=(5, 3, 256, 256)).double()\n",
    "output_3 = IN_layer_3(input)\n",
    "output_4 = IN_layer_4(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[101.,  81., 182.,  ..., 148., 174.,  49.],\n",
       "          [203.,  47., 250.,  ..., 130.,  10., 182.],\n",
       "          [  1.,  95., 101.,  ..., 190., 132., 142.],\n",
       "          ...,\n",
       "          [ 33.,  27.,  21.,  ..., 234.,  98., 185.],\n",
       "          [127.,  78.,  64.,  ...,  30., 184.,   1.],\n",
       "          [ 31., 137., 165.,  ...,  72., 213., 252.]],\n",
       "\n",
       "         [[114., 129., 114.,  ...,  13.,  24.,  77.],\n",
       "          [ 83., 183., 183.,  ..., 153., 131., 226.],\n",
       "          [ 52., 200., 113.,  ..., 104., 216.,  32.],\n",
       "          ...,\n",
       "          [231.,  48.,  33.,  ..., 184., 188.,  82.],\n",
       "          [202., 118.,  10.,  ...,  46.,   9., 253.],\n",
       "          [ 75., 141.,  39.,  ...,  65., 107.,  97.]],\n",
       "\n",
       "         [[  8.,   9., 128.,  ...,  58., 160., 177.],\n",
       "          [138., 250.,  95.,  ..., 220.,  67.,  40.],\n",
       "          [202., 155.,  25.,  ..., 169., 191., 103.],\n",
       "          ...,\n",
       "          [188.,  96.,  39.,  ..., 211., 117.,  90.],\n",
       "          [163., 146.,  61.,  ..., 104., 186., 203.],\n",
       "          [193.,  88.,  30.,  ..., 121., 121., 140.]]],\n",
       "\n",
       "\n",
       "        [[[ 98.,  36.,  89.,  ...,  65.,  16.,  46.],\n",
       "          [ 53., 142.,  35.,  ..., 188.,   6.,  45.],\n",
       "          [ 69., 219., 237.,  ...,  23., 101.,  34.],\n",
       "          ...,\n",
       "          [151., 224.,  30.,  ..., 209.,  30., 132.],\n",
       "          [ 51., 168., 158.,  ...,   1.,  65., 198.],\n",
       "          [  4.,  75., 107.,  ..., 115., 231.,  44.]],\n",
       "\n",
       "         [[ 17., 149., 232.,  ..., 163., 190., 209.],\n",
       "          [  2.,  23., 128.,  ..., 210., 123., 181.],\n",
       "          [ 87., 135., 165.,  ...,  81., 237.,  92.],\n",
       "          ...,\n",
       "          [214., 212.,  63.,  ..., 212., 147., 223.],\n",
       "          [243.,  23., 160.,  ..., 201.,  77.,  15.],\n",
       "          [133.,  54., 154.,  ..., 252.,  82., 254.]],\n",
       "\n",
       "         [[140., 238., 100.,  ...,  12., 248., 244.],\n",
       "          [  0.,  66.,  43.,  ...,  98., 146., 241.],\n",
       "          [211., 245., 172.,  ...,  90., 149.,  34.],\n",
       "          ...,\n",
       "          [ 86., 126.,  15.,  ...,  10.,  19.,  87.],\n",
       "          [254., 173.,  44.,  ...,  11., 234.,  44.],\n",
       "          [ 41., 202., 232.,  ..., 125., 117.,  33.]]],\n",
       "\n",
       "\n",
       "        [[[ 61., 131., 141.,  ..., 241., 121.,  11.],\n",
       "          [200., 213.,  42.,  ..., 216.,   8., 137.],\n",
       "          [145.,  92., 184.,  ..., 143., 167., 150.],\n",
       "          ...,\n",
       "          [195.,  10., 151.,  ...,  16., 187., 140.],\n",
       "          [198.,  90., 159.,  ..., 170.,  67.,  26.],\n",
       "          [ 26.,  24., 248.,  ...,  70., 241., 248.]],\n",
       "\n",
       "         [[ 19.,  86., 213.,  ...,  73.,  50.,  28.],\n",
       "          [139.,  53., 126.,  ..., 160., 248., 239.],\n",
       "          [195., 191., 198.,  ...,  72.,  65.,  14.],\n",
       "          ...,\n",
       "          [162.,  71., 163.,  ..., 107., 253., 141.],\n",
       "          [ 37., 242., 227.,  ...,  22., 123., 254.],\n",
       "          [129.,   1., 175.,  ..., 252., 206., 174.]],\n",
       "\n",
       "         [[ 16., 216., 175.,  ..., 173., 200., 197.],\n",
       "          [ 17., 226.,  12.,  ..., 206.,  98., 211.],\n",
       "          [ 65.,  18.,  38.,  ..., 186.,  71.,  73.],\n",
       "          ...,\n",
       "          [172.,  14., 126.,  ..., 119., 102.,  68.],\n",
       "          [ 23.,  29., 162.,  ..., 164., 229.,  38.],\n",
       "          [149., 198., 151.,  ...,  95.,  60.,  88.]]],\n",
       "\n",
       "\n",
       "        [[[181.,   9.,  89.,  ..., 106., 229., 221.],\n",
       "          [  5.,  62.,  17.,  ...,  24., 236., 131.],\n",
       "          [ 10., 167., 236.,  ...,  54.,  41.,  26.],\n",
       "          ...,\n",
       "          [203.,  95., 111.,  ..., 207.,  55.,  98.],\n",
       "          [ 69., 216., 198.,  ..., 146.,  44.,   8.],\n",
       "          [ 92.,  17., 141.,  ...,  97., 118., 168.]],\n",
       "\n",
       "         [[129., 248., 197.,  ..., 130., 171., 162.],\n",
       "          [253., 126.,  40.,  ...,  45., 127., 193.],\n",
       "          [125., 245.,  50.,  ..., 152., 157., 197.],\n",
       "          ...,\n",
       "          [217.,   5., 124.,  ..., 173.,  76., 141.],\n",
       "          [156., 109., 130.,  ...,  97., 252.,  30.],\n",
       "          [182.,  19., 239.,  ...,  26.,  63., 205.]],\n",
       "\n",
       "         [[250.,  31.,  22.,  ..., 208.,  30., 147.],\n",
       "          [113., 142., 211.,  ...,  60., 214., 111.],\n",
       "          [230., 122.,  49.,  ..., 216.,  19., 242.],\n",
       "          ...,\n",
       "          [ 58., 252., 166.,  ..., 161.,  23.,  81.],\n",
       "          [159.,   3.,  16.,  ..., 218., 224.,  82.],\n",
       "          [218., 245.,  62.,  ...,  83.,  57.,  67.]]],\n",
       "\n",
       "\n",
       "        [[[ 10.,  55.,  61.,  ...,  67.,  77., 124.],\n",
       "          [ 23.,  42., 198.,  ...,  28.,  67.,   2.],\n",
       "          [198., 199., 123.,  ..., 117., 140.,  99.],\n",
       "          ...,\n",
       "          [175., 244., 204.,  ...,   6., 189., 109.],\n",
       "          [ 48., 188., 193.,  ..., 124.,   5., 141.],\n",
       "          [208.,  89.,  92.,  ..., 136., 216.,  24.]],\n",
       "\n",
       "         [[ 72.,  43.,  79.,  ...,  33., 132., 152.],\n",
       "          [ 18., 253., 170.,  ...,  59., 142., 197.],\n",
       "          [105., 207.,  62.,  ..., 147., 103.,  97.],\n",
       "          ...,\n",
       "          [216., 170.,  40.,  ...,   0., 230.,  42.],\n",
       "          [241.,  29., 249.,  ..., 131., 165., 112.],\n",
       "          [175.,  69.,   6.,  ...,  86., 142., 102.]],\n",
       "\n",
       "         [[123., 148., 189.,  ...,  65., 234., 229.],\n",
       "          [185., 188., 183.,  ...,  87., 228.,  59.],\n",
       "          [ 68.,  80., 179.,  ...,  98., 237., 155.],\n",
       "          ...,\n",
       "          [140., 101.,   9.,  ..., 115.,  84., 179.],\n",
       "          [229.,  74., 108.,  ..., 190., 155., 231.],\n",
       "          [ 51.,  31.,  53.,  ..., 127.,  64.,  37.]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]; Torch version: 2.3.0.post100\n",
      "2 tensor(True)\n",
      "3 tensor(True)\n",
      "4 tensor(True)\n",
      "5 tensor(True)\n",
      "6 tensor(True)\n",
      "7 tensor(True)\n",
      "8 tensor(True)\n",
      "9 tensor(True)\n",
      "10 tensor(True)\n",
      "11 tensor(True)\n",
      "12 tensor(True)\n",
      "13 tensor(True)\n",
      "14 tensor(True)\n",
      "15 tensor(True)\n",
      "16 tensor(True)\n",
      "17 tensor(True)\n",
      "18 tensor(True)\n",
      "19 tensor(True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n"
     ]
    }
   ],
   "source": [
    "## What is the purpose of num_features in `nn.InstanceNorm2d`?\n",
    "##   Seems like from the warning message provided during cases when the\n",
    "##   input channel count and num_features is mismatched, `affine` is false\n",
    "##   thus the `num_features` is ignored.\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import sys\n",
    "\n",
    "# print(f\"Python version: {sys.version}; Torch version: {torch.__version__}\")\n",
    "\n",
    "# input = torch.randint(low=0, high=255, size=(5, 3, 256, 256)).double()\n",
    "# output_base = nn.InstanceNorm2d(num_features=1)(input)\n",
    "\n",
    "# for i in range(2, 20):\n",
    "#     IN_layer = nn.InstanceNorm2d(num_features=i)\n",
    "    \n",
    "#     output = IN_layer(input)\n",
    "#     result_check_same = (output_base == output).all()\n",
    "    \n",
    "#     print(i, result_check_same)\n",
    "\n",
    "#     if not result_check_same:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Input images have to be 256x256, got 260x256",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m255\u001b[39m, (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m260\u001b[39m, \u001b[38;5;241m256\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      5\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m gan\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mGenerator()\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtest_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/github/kaggle_kernels/gan/src/gan/generator.py:151\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m:torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Generator__input_size_okay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Unet downsampling steps\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     skips_holder \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/github/kaggle_kernels/gan/src/gan/generator.py:145\u001b[0m, in \u001b[0;36mGenerator._Generator__input_size_okay\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Generator__input_size_okay\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m:torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Checks if input dimension is acceptable.\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mmatch\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim():\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    139\u001b[0m             dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (dim[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m256\u001b[39m) \u001b[38;5;241m|\u001b[39m (dim[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m    141\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput images have to be 256x256, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    143\u001b[0m             dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    144\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (dim[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m256\u001b[39m) \u001b[38;5;241m|\u001b[39m (dim[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m--> 145\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput images have to be 256x256, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect input dimension size of either 3 or 4, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Input images have to be 256x256, got 260x256"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gan.generator\n",
    "\n",
    "input = torch.randint(255, (10, 3, 260, 256)).float()\n",
    "test_generator = gan.generator.Generator()\n",
    "output = test_generator(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (double) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m in_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(in_channels, out_channels, kernel_size)\u001b[38;5;66;03m#.double()\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ds/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (double) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input = torch.randint(255, (5, 255, 255)).double()\n",
    "kernel_size = 4\n",
    "out_channels = 3\n",
    "in_channels = input.shape[0]\n",
    "layer = nn.Conv2d(in_channels, out_channels, kernel_size)#.double()\n",
    "output = layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import gan.generator\n",
    "import torch\n",
    "\n",
    "n_filters=3\n",
    "input = torch.randint(255, (3, 256, 256)).float()\n",
    "layer = gan.generator.Downsampler(filters=n_filters, kernel_size=4)\n",
    "output = layer(input)\n",
    "output = layer(output)\n",
    "output = layer(output)\n",
    "output = layer(output)\n",
    "output = layer(output)\n",
    "output = layer(output)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.size(): torch.Size([5, 3, 256, 256])\n",
      "output.size(): torch.Size([5, 5, 254, 254])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "input = torch.randint(0, 10, size=(5, 3, 256, 256)).float()\n",
    "layer = nn.Conv2d(in_channels=input.size()[1], out_channels=5, kernel_size=3, stride=1)\n",
    "output = layer(input)\n",
    "\n",
    "print(f\"input.size(): {input.size()}\")\n",
    "print(f\"output.size(): {output.size()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
