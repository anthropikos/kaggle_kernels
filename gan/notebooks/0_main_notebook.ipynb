{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN Project Write-up\n",
    "\n",
    "Anthony Lee 2025-01-06\n",
    "\n",
    "## Abstract\n",
    "This project created a Generative Adversarial Network (GAN) as proposed by Ian Goodfellow (Goodfellow et al., 2014) with the generator network inspired by the U-Net architecture and the discriminator to be a simple multi-level perceptron (MLP). The choice of U-Net architecture is to utilize its ability augment data and capture the images' context with relatively few available data. With limited number of Monet data available in contrast the infinitely limitless non-Monet images available, this is a very desireable trait. The GAN also leverage cycled images to further augment the limited number of training data inspired by Amy Jang's CycleGAN tutorial on Kaggle (Monet CycleGAN Tutorial, n.d.). The model is evaluated as part of the Kaggle's ongoing competition \"I'm Something of a Patiner Myself\" using a modified Frechet Inception Distance (FID) (Bioinf-Jku/TTUR, 2017/2024). The model performance received a score of 145.49744 from approximatley 7000 generated Monets after training for 10 epochs. For reference, the best performing model on the leadership board received a score of 33.82955 at this moment of writing.\n",
    "\n",
    "## Introduction\n",
    "Happy new year!\n",
    "In 2014, Ian Goodfellow published a framework of machine learning model and termed it Generative Adversarial Net (GAN), which is composed of a generator and a discriminator. The generator and discriminator are both deep multi-level perceptron models capable of backpropagations. A unique solution for this framework exists when the generator `G` able to recover the training data's distribution and the discriminator `D` equal to `1/2` everywhere (Goodfellow et al., 2014). The elimination of Markov chain sidesteps the issue faced by intractable likelihood functions in much more complex models.\n",
    "\n",
    "## Method\n",
    "In this project I aimed to implement my first GAN instead of focusing too much on model performance and optimizations. Additionally, practicing ways to organize my code and diagram to assist with my own understanding of the complex model.\n",
    "\n",
    "<div><img src=\"../CycleGAN_Process.png\" width=\"800\" sytle=\"display: block; margin-left: auto; margin-right: auto;\"/></div>\n",
    "\n",
    "The model is implemented using PyTorch and closely emulates that of the Amy Jang's model besides some minor changes due to the time cost for training each model. At a high level, there is a total of four models being trained and backpropaged, and they are the \"monet-generator\", \"photo-generator\", \"monet-discriminator\", and \"photo-discriminator.\"\n",
    "\n",
    "## Discussion\n",
    "\n",
    "## Future Improvements\n",
    "- Transfer learning\n",
    "## Conclusion\n",
    "\n",
    "## References\n",
    "Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative Adversarial Nets. Advances in Neural Information Processing Systems, 27. https://proceedings.neurips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html\n",
    "\n",
    "Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation (No. arXiv:1505.04597). arXiv. https://doi.org/10.48550/arXiv.1505.04597\n",
    "\n",
    "Monet CycleGAN Tutorial. (n.d.). Retrieved January 6, 2025, from https://kaggle.com/code/amyjang/monet-cyclegan-tutorial\n",
    "\n",
    "Bioinf-jku/TTUR. (2024). [Jupyter Notebook]. Institute of Bioinformatics, Johannes Kepler University Linz. https://github.com/bioinf-jku/TTUR (Original work published 2017)\n",
    "\n",
    "\n",
    "\n",
    "## Appendix\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "### Image data type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
