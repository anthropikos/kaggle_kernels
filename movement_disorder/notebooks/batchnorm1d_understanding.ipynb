{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding what BatchNorm1d does\n",
    "Anthony Lee 2025-02-13\n",
    "\n",
    "It was not very clear what [the PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) meant by \"the mean and standard-deviation are calculated per-dimension over the mini-batches\", thus this is an empirical test to understand it.\n",
    "\n",
    "From the test below, `BatchNorm1d` has the same result as when I calculate the mean and variance at dimension 0 and 2, which corresponds to batches and \"for each sample within each of its dimensions\".\n",
    "\n",
    "### \"per-dimension\" == \"for each sample within each of its dimensions\"\n",
    "One can think of the data to be represented in a 3D tensor with x being batches, y being channels, and z being the data length. When we take the mean or variance \"per-dimension\", we should have a mean for each channel and each item in the batch (i.e., x-axis). \n",
    "\n",
    "### \"over the mini-batches\"\n",
    "When then further taking the mean of these \"per-dimension\" statistics, we will be taking the average of all the statistics with the same batch, thus we should have a vector of means that is the length of the number of channels because we are taking the mean \"over\" or \"across\" the batch/mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([5, 3, 5])\n",
      "\n",
      "BatchNorm1d output shape: torch.Size([5, 3, 5])\n",
      "tensor([[[ 1.8643, -1.4489, -1.3801, -0.3188,  0.4848],\n",
      "         [ 1.7870, -0.7558, -0.0987,  0.0321,  1.2541],\n",
      "         [ 1.7270,  0.0469, -1.4423, -0.0950, -0.0756]],\n",
      "\n",
      "        [[-0.9905,  0.3496,  0.8653, -0.7460,  0.0162],\n",
      "         [-0.7105, -0.7918, -0.9328,  0.8435, -0.1628],\n",
      "         [-1.4659,  0.3299,  0.1079,  1.3647, -0.1284]],\n",
      "\n",
      "        [[-1.2634, -0.7957, -0.6404,  0.2914,  2.0487],\n",
      "         [-1.3422,  1.1924, -0.3426,  0.0965, -0.5516],\n",
      "         [-0.5273, -0.2496, -0.3470,  1.4835,  1.0654]],\n",
      "\n",
      "        [[-0.6816,  0.7755,  0.0096, -0.4295, -0.4612],\n",
      "         [-0.7923,  1.6722, -0.8551,  0.9907, -1.3649],\n",
      "         [ 1.4516, -0.0489,  0.5959, -1.4507,  0.3446]],\n",
      "\n",
      "        [[-0.6476,  0.2967,  2.1950, -0.3604,  0.9670],\n",
      "         [ 0.1861,  0.6920, -1.1769,  1.9294, -0.7983],\n",
      "         [-1.2574, -1.2964, -0.3403,  1.4980, -1.2907]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Generate some example data\n",
    "batch_size, n_channel, data_length = 5, 3, 5\n",
    "eps = 1e-9\n",
    "\n",
    "input = torch.randint(low=int(1e1), high=int(1e5), size=(batch_size, n_channel, data_length), dtype=torch.float32)\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print()\n",
    "\n",
    "# The BatchNorm1d function\n",
    "norm = torch.nn.BatchNorm1d(num_features=n_channel, eps=eps)\n",
    "output = norm(input)\n",
    "\n",
    "print(f\"BatchNorm1d output shape: {output.shape}\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and variances with the keep dim:\n",
      "\tmean:\ttensor([[[40511.1992],\n",
      "         [41540.0391],\n",
      "         [46157.1211]]])\n",
      "\tvar:\ttensor([[[5.7631e+08],\n",
      "         [9.1064e+08],\n",
      "         [7.3417e+08]]])\n",
      "\n",
      "The dimensions below should match that of the 1th dimension of the input, which is 3\n",
      "mean and variance without keepdim=True:\n",
      "\tmean:\ttensor([40511.1992, 41540.0391, 46157.1211])\n",
      "\tvar:\ttensor([5.7631e+08, 9.1064e+08, 7.3417e+08])\n",
      "\n",
      "Keeping the dimension just makes the matrix calculations easier.\n",
      "Output shape: torch.Size([5, 3, 5])\n",
      "tensor([[[ 1.8266, -1.4196, -1.3522, -0.3124,  0.4750],\n",
      "         [ 1.7509, -0.7405, -0.0967,  0.0314,  1.2287],\n",
      "         [ 1.6921,  0.0459, -1.4131, -0.0931, -0.0740]],\n",
      "\n",
      "        [[-0.9705,  0.3426,  0.8478, -0.7309,  0.0159],\n",
      "         [-0.6962, -0.7758, -0.9139,  0.8265, -0.1595],\n",
      "         [-1.4363,  0.3233,  0.1057,  1.3371, -0.1258]],\n",
      "\n",
      "        [[-1.2379, -0.7796, -0.6275,  0.2855,  2.0073],\n",
      "         [-1.3151,  1.1683, -0.3357,  0.0946, -0.5405],\n",
      "         [-0.5166, -0.2445, -0.3399,  1.4536,  1.0439]],\n",
      "\n",
      "        [[-0.6679,  0.7598,  0.0094, -0.4208, -0.4519],\n",
      "         [-0.7763,  1.6384, -0.8378,  0.9707, -1.3373],\n",
      "         [ 1.4223, -0.0479,  0.5839, -1.4214,  0.3377]],\n",
      "\n",
      "        [[-0.6345,  0.2907,  2.1506, -0.3531,  0.9474],\n",
      "         [ 0.1823,  0.6780, -1.1531,  1.8905, -0.7821],\n",
      "         [-1.2320, -1.2702, -0.3335,  1.4677, -1.2646]]])\n"
     ]
    }
   ],
   "source": [
    "# Average across batches (0) and per-dimension (2)\n",
    "dim = [0, 2]  # Across batches and per-dimension\n",
    "\n",
    "mean = input.mean(dim=dim, keepdim=True)\n",
    "variance = input.var(dim=dim, keepdim=True)\n",
    "\n",
    "output = (input - mean) / torch.sqrt(variance + eps)\n",
    "\n",
    "print(\"Mean and variances with the keep dim:\")\n",
    "print(f\"\\tmean:\\t{mean}\\n\\tvar:\\t{variance}\")\n",
    "print()\n",
    "\n",
    "print(f\"The dimensions below should match that of the 1th dimension of the input, which is {n_channel}\")\n",
    "print(f\"mean and variance without keepdim=True:\\n\\tmean:\\t{input.mean(dim=dim, keepdim=False)}\\n\\tvar:\\t{input.var(dim=dim, keepdim=False)}\")\n",
    "print()\n",
    "\n",
    "print(\"Keeping the dimension just makes the matrix calculations easier.\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movement_disorder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
